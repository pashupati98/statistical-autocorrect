{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View on Kaggle - [Notebook](https://www.kaggle.com/pashupatigupta/project-statistical-auto-correct-system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5c6ba0f9-cccf-46be-bb3a-1cfe081456e2",
    "_uuid": "0f146f53-b091-4589-b851-b0985e83e446"
   },
   "source": [
    "# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Introduction</p>\n",
    "\n",
    "I'm often amazed by the auto-correct systems that are used by Google, Grammerly, Android etc. I wanted to know how these systems work and I started reading about them. And as expected...They are really complex!! If you too want to read about them. Here you go - \n",
    "- [Using the Web for Language Independent Spellchecking and\n",
    "Autocorrection](http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36180.pdf)\n",
    "- [How Difficult is it to Develop a Perfect Spell-checker?\n",
    "A Cross-linguistic Analysis through Complex Network Approach](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=52A3B869596656C9DA285DCE83A0339F?doi=10.1.1.146.4390&rep=rep1&type=pdf)\n",
    "\n",
    "> Then I found an intutive auto correct system that uses statistics, probability and dynamic programming. In this project I have tried to impliment this auto-correct system. [References at the bottom]\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d9892c4a-1471-4bbb-b103-ba86f2a6b214",
    "_uuid": "e32da8ad-8c1b-4549-a2e5-a0f9f890e3e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4be18ca3-c2ce-4e56-8cbb-8db3a9ea91c2",
    "_uuid": "2f64ee38-5d8b-48ef-9a82-a7de450cd04b"
   },
   "source": [
    "# Auto-correct systems\n",
    "<div>\n",
    "<p>The task of an auto-correct system is finding out which words in a document are misspelled. These mispelled words might be presented to a user by underlining that words. Correction is the task of substituting the well-spelled word for misspellings.\n",
    "</p>\n",
    "<img style=\"align:center\", src=\"https://github.com/pashupati98/kaggle-archives/blob/main/img/img2.PNG?raw=true\">\n",
    "    <hr>\n",
    "    <p>The very first requirement of auto-correct system is data. I have checked multiple data sources and will be using one of them.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bfc9674d-9fc3-4f68-a061-a304b67ea8e6",
    "_uuid": "ff854229-7e47-47a6-887b-6be58c098f26"
   },
   "source": [
    "## Data Requirement\n",
    "\n",
    "We need a trusted text corpus that we'll use to build the auto-correct system. There are many public domain text corpus. Since it's a unsupervised type of problem here what we need is just text. You can use any competition data or any other public dataset that has text field column. In the currect version I have used subset of wikipedia corpus.\n",
    "\n",
    "Let's look at the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a26d2e83-7ecd-4b31-86aa-493847ef6e84",
    "_uuid": "c8b4fd06-c10c-46e6-b5e9-9a78b763fbca"
   },
   "outputs": [],
   "source": [
    "with open('../input/text-corpus/wiki_corpus.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    file = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc9a8c32-c6fb-4f4f-9fd5-4f0b4e6d771b",
    "_uuid": "38323031-0870-451d-a24e-97f19510bff4"
   },
   "outputs": [],
   "source": [
    "# A small sample of the corpus\n",
    "file[0][:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a9ae404-924c-4e8d-942e-3864fa76f724",
    "_uuid": "0739dbe8-a6be-4586-b903-bb08fbcc718f"
   },
   "source": [
    "Cool. Now we need to process this corpus. Since it's pretty clean corpus we need to do only two thisga - Tokenizing and Lowercasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b825f32a-99c3-4e34-9722-a7f68aa0b700",
    "_uuid": "50214802-dcf3-4916-a907-71fca3f52080"
   },
   "outputs": [],
   "source": [
    "def process_data(lines):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    words = []        \n",
    "    for line in lines:\n",
    "        line = line.strip().lower()\n",
    "        word = re.findall(r'\\w+', line)\n",
    "        words.extend(word)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8098e71-b9a8-4d7c-8ca8-a7245314aca6",
    "_uuid": "b571aa84-df60-4ee6-9dd9-10b552fae62c"
   },
   "outputs": [],
   "source": [
    "word_l = process_data(file)\n",
    "vocab = set(word_l)\n",
    "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "46b86ff0-00e9-41c6-9488-32ae0b34f8f5",
    "_uuid": "12112af1-5bee-4013-a015-88cbad24846f"
   },
   "source": [
    "The data looks fine. Before moving to the next step let's first look at the architectire of our syste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14ccf0e9-b208-46bb-afad-2e52fad4577f",
    "_uuid": "1d94b12b-7d76-4350-8e67-60431c41c41a"
   },
   "source": [
    "# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Architecture</p>\n",
    "\n",
    "<div>\n",
    "<img style=\"align:center\", src=\"https://github.com/pashupati98/kaggle-archives/blob/main/img/architecture.png?raw=true\">\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "This auto-correct architecture has 4 components -\n",
    "- 1) Filtering Mispells : One simple approach could be checking if a word is there in the vocabulary or not. \n",
    "- 2) Word Suggestion Mechanism : This mechnism suggests candidate words based on deletion, insertion, switch or replace of one/two characters in the original word.\n",
    "- 3) Probability Distribution Mechanism : The probability distribution {key(word) : value(probability)} is created calculated using a large text corpus. Probability of each candidate is found using this distribution and the most probable candidate is the final one.\n",
    "- 4) Replace Mispells : Simple replace the mispelled word with the most probable suggestion.\n",
    "\n",
    "We'll impliment each part separetely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3e01f2d9-ad72-483e-9123-f5b58966def7",
    "_uuid": "25bf49e7-91be-41cb-b96f-d60f7244569f"
   },
   "source": [
    "### Artchitecture Part 1 : (Filtering Mispells)\n",
    "\n",
    "A function that tokenizes the sentences and checks the availability of each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "130d9a13-5fab-4b9c-b5bc-fc81582aeb96",
    "_uuid": "d24c9457-6c07-43f9-ab66-df209f9f0e62"
   },
   "outputs": [],
   "source": [
    "def find_wrong_word(sent, vocab):\n",
    "    wrong_words = []\n",
    "    sent = sent.strip().lower().split(\" \")\n",
    "    for word in sent:\n",
    "        if word not in vocab:\n",
    "            wrong_words.append(word)\n",
    "    return wrong_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4f7d0d10-f04c-4b14-b339-b9378f06f0b7",
    "_uuid": "4fc857a5-1633-4d6d-b47d-ea158d725fef"
   },
   "outputs": [],
   "source": [
    "find_wrong_word('he is goinng home', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2f57a1c0-7c6f-4e28-b7e7-16201a68a5b1",
    "_uuid": "61939ae7-47da-4f0a-89b3-24f7d8e34cdb"
   },
   "source": [
    "### Architecture Part 2 : (Word Suggestion Mechanism)\n",
    "\n",
    "We'll impliment separate functions of each of the steps (deletion, insertion, switching, replace) and then combine them to edit one or two letter of the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a8f5ec9-3a0b-4c6d-bf3d-f6bdfbed643e",
    "_uuid": "c8cbe5e1-ea82-4a34-8c0e-780332ab55b6"
   },
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    delete_l = [s[0]+s[1][1:] for s in split_l]\n",
    "    if verbose: print(f\"input word : {word} \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "227c4c9b-a34f-471c-abc9-0c61ca2eae00",
    "_uuid": "d4b5da59-069c-4894-a51b-79b4215039e0"
   },
   "outputs": [],
   "source": [
    "delete_word_l = delete_letter(word=\"cans\",\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e0b3e1f2-3fb3-4b49-8b7e-dd7ebfac946b",
    "_uuid": "c6366fe7-314c-48bd-befd-cd2abb87e04d"
   },
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    for s in split_l:\n",
    "        if len(s[1])>2:\n",
    "            temp = s[0] + s[1][1] + s[1][0] + s[1][2:]\n",
    "        elif len(s[1]) == 2:\n",
    "            temp = s[0] + s[1][1] + s[1][0]\n",
    "        elif len(s[1]) == 1:\n",
    "            continue\n",
    "        switch_l.append(temp)\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "\n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5576ec3e-34ea-40bc-b710-a83fcdffd8ce",
    "_uuid": "7b9b4583-bf2d-4e78-9e81-42a1b82b4498"
   },
   "outputs": [],
   "source": [
    "switch_word_l = switch_letter(word=\"eta\",\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bfd0b1f0-5aba-42fe-a692-fd5a10caf088",
    "_uuid": "35b3664a-2fc1-4549-b7dc-09ccfe71b52c"
   },
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    for s in split_l:\n",
    "        if len(s[1]) == 1:\n",
    "            for l in letters:\n",
    "                if l != s[1][0]:\n",
    "                    temp = l\n",
    "                    replace_l.append(s[0]+temp)\n",
    "        elif len(s) > 1:\n",
    "            for l in letters:\n",
    "                if l != s[1][0]:\n",
    "                    temp = l + s[1][1:]\n",
    "                    replace_l.append(s[0]+temp)\n",
    "        \n",
    "    replace_set = set(replace_l)\n",
    "    \n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "015a3d2e-b099-4fc0-ac63-fcbc19db08ff",
    "_uuid": "e9b20be5-e42f-4575-b551-1bdd891c8c05"
   },
   "outputs": [],
   "source": [
    "replace_l = replace_letter(word='can',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f4666f80-8a44-428d-8a49-c7fce6df504f",
    "_uuid": "a6be624c-71b3-4f91-aa75-ff72f76db097"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of outputs of switch_letter('at') is {len(switch_letter('fate'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "960708b0-22e9-4404-bf72-8acb0ba0377b",
    "_uuid": "7a1f34ca-e16f-49b3-bc4e-59b68588173b"
   },
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    for s in split_l:\n",
    "        for l in letters:\n",
    "            insert_l.append(s[0]+l+s[1])\n",
    "\n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19304af7-6e72-4157-8634-e326bba8d5fa",
    "_uuid": "d549ee37-2b3f-48f1-89e9-95790a725724"
   },
   "outputs": [],
   "source": [
    "insert_l = insert_letter('at', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78fc3726-e429-4873-837c-2e2744744faa",
    "_uuid": "d8bb934e-5bda-4e63-8b56-28a42a2e08ae"
   },
   "source": [
    "#### Let's combine these individual steps and impliment two function for for editing on or two characters from a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8a7fca77-dad3-4ffd-82ae-4e67b798e725",
    "_uuid": "20f83bb1-0696-45c5-9d73-25c92e53e466"
   },
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "    insert_l = insert_letter(word)\n",
    "    delete_l = delete_letter(word)\n",
    "    replace_l = replace_letter(word)\n",
    "    switch_l = switch_letter(word)\n",
    "    \n",
    "    if allow_switches:\n",
    "        ans = insert_l + delete_l + replace_l + switch_l\n",
    "    else:\n",
    "        ans = insert_l + delete_l + replace_l\n",
    "        \n",
    "    edit_one_set = set(ans)\n",
    "\n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "319241ff-8dde-4c44-8de3-636813289841",
    "_uuid": "862574ab-c289-454f-8fe8-300c12669eeb"
   },
   "outputs": [],
   "source": [
    "tmp_word = \"at\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word : {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "#print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68a082be-6081-4bb5-a159-b676c945d6e1",
    "_uuid": "a75648bc-63fa-427c-a7dc-97a6ed56f4ec"
   },
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "    one_edit = edit_one_letter(word)\n",
    "    ans = []\n",
    "    for w in one_edit:\n",
    "        ans.append(w)\n",
    "        ans.extend(edit_one_letter(w))\n",
    "        \n",
    "    edit_two_set = set(ans)\n",
    "    \n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ef00a090-6609-4648-b1e0-99a2445f8b72",
    "_uuid": "d7ab7d83-10e2-4282-8d86-3679a15bde94"
   },
   "outputs": [],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "149b2f48-5586-4398-95bf-9d2944cdd409",
    "_uuid": "3faa34f7-c566-4606-8684-f13caf408bea"
   },
   "source": [
    "We have the second part implimented and it is working pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc712ba8-3324-4349-a77d-708dd182765e",
    "_uuid": "8cccb753-4a5a-4f09-ac7a-2c300364af17"
   },
   "source": [
    "### Architecture Part 3 : (Probability Distribution)\n",
    "\n",
    "We'll calculate the frequecies of each word using the corpus that we have. Then we'll divide each frequencies by word count to find the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eeea77bb-36e4-4026-be5a-475b7173b0c7",
    "_uuid": "f0ae3f71-ba6d-4e24-b030-d19d35b0f857"
   },
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    word_count_dict = {}  \n",
    "    word_count_dict = Counter(word_l)\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "70be75dd-3dcf-48e8-a90a-0c297f3d5217",
    "_uuid": "b4adfb36-4f1d-42c5-ad3b-0a9ef1fa8584"
   },
   "outputs": [],
   "source": [
    "word_count_dict = get_count(word_l)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e9a34b7-e3bd-461f-8ab8-28996863f843",
    "_uuid": "f93a1215-e1ba-485d-9c16-6706aa873044"
   },
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {} \n",
    "    total = 1\n",
    "    for word in word_count_dict.keys():\n",
    "        total = total + word_count_dict[word]\n",
    "        \n",
    "    for word in word_count_dict.keys():\n",
    "        probs[word] = word_count_dict[word]/total\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fbf09122-188b-46f7-964a-4ec061c1b1d1",
    "_uuid": "127655f7-a94a-4bcb-88d4-21e18ae3b8d0"
   },
   "outputs": [],
   "source": [
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('you') is {probs['you']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c74f1f53-2b4d-49be-b3dd-00e82d902cc6",
    "_uuid": "6807bdec-1c46-4edc-93e2-49496d5da68b"
   },
   "source": [
    "Let's look at the probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f61e91f2-5c3a-423a-95e1-61dcc41adace",
    "_uuid": "d65fd0bb-6f2a-4df9-9797-5488eeb0783e"
   },
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame({'word':probs.keys(), 'probability':probs.values()}).sort_values(by='probability', ascending=False)\n",
    "prob_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8b97ce74-8cf4-4a1e-8c7d-3efbf4e5fa72",
    "_uuid": "03fa7a2e-29f0-4509-b76a-50d2dac22fab"
   },
   "outputs": [],
   "source": [
    "prob_df.head().plot.bar(x='word', y='probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29302848-b19e-4070-8031-916315f9ff74",
    "_uuid": "2382e3af-64bd-48e7-919a-1ed8f71bf8d3"
   },
   "source": [
    "Looks as expected! When we'll used larger corpus then we'll have more accurate probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a01fabfa-68c8-48ac-9118-208e6e33cdce",
    "_uuid": "be2dfcf6-5b2c-4b49-bdc9-c6fa5b6fe2a2"
   },
   "source": [
    "### Architecture Part 4 : (Replace Misspells with correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c56ee50c-f87b-4fe8-b143-7ed6bcb53171",
    "_uuid": "93c61764-5114-443a-bbf2-7302bbc0a580"
   },
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "   \n",
    "    if word in probs.keys():\n",
    "        suggestions.append(word)\n",
    "    for w in edit_one_letter(word):\n",
    "        if len(suggestions) == n:\n",
    "            break\n",
    "        if w in probs.keys():\n",
    "            suggestions.append(w)\n",
    "    for w in edit_two_letters(word):\n",
    "        if len(suggestions) == n:\n",
    "            break\n",
    "        if w in probs.keys():\n",
    "            suggestions.append(w)\n",
    "        \n",
    "    best_words = {}\n",
    "    \n",
    "    for s in suggestions:\n",
    "        best_words[s] = probs[s]\n",
    "        \n",
    "    best_words = sorted(best_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    n_best = best_words \n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f7b5bb43-9f88-4c1d-a3f5-4eeb6dd0ab19",
    "_uuid": "e7e5f53e-64cc-4f91-adaf-2c9719d00444"
   },
   "outputs": [],
   "source": [
    "def get_correct_word(word, vocab, probs, n): \n",
    "    corrections = get_corrections(word, probs, vocab, n, verbose=False)\n",
    "#    print(corrections)\n",
    "    if len(corrections) == 0:\n",
    "        return word\n",
    "    \n",
    "    final_word = corrections[0][0]\n",
    "    final_prob = corrections[0][1]\n",
    "    for i, word_prob in enumerate(corrections):\n",
    "        #print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "        if word_prob[1] > final_prob:\n",
    "            final_word = word_prob[0]\n",
    "            final_prob = word_prob[1]\n",
    "    return final_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a63b515-0dd5-4910-84c3-8ace7ef624d0",
    "_uuid": "146deb16-036e-453e-8cd7-545bb0dd9891"
   },
   "outputs": [],
   "source": [
    "get_correct_word('annd', vocab, probs, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f3184d38-7dc0-4cea-875f-f5fb2d1dd29a",
    "_uuid": "6de4c543-9f2e-488a-823f-5e54bb6ecc3f"
   },
   "source": [
    "#### All Done!! Let's wrap up everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e8b7d44e-bc4e-485a-bc68-1912b4d0b178",
    "_uuid": "91b33868-98ec-40a1-9e8c-33a7a3bea556"
   },
   "outputs": [],
   "source": [
    "def autocorrect(sentence, vocab, probs):\n",
    "    print(\"Input sentence : \", sentence)\n",
    "    wrong_words = find_wrong_word(sentence, vocab)\n",
    "    print(\"Wrong words : \", wrong_words)\n",
    "    #print(wrong_words)\n",
    "    correct_words = []\n",
    "    for word in sentence.strip().lower().split(\" \"):\n",
    "        if word in wrong_words:\n",
    "            correct_word = get_correct_word(word, vocab, probs, 15)\n",
    "            #print(word, correct_word)\n",
    "            word = correct_word\n",
    "        correct_words.append(word)\n",
    "    print(\"Output Sentence : \", \" \".join(correct_words).capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "18d14e9d-69c7-47fd-ab20-fd646b16964f",
    "_uuid": "e0aa2bef-f968-4b03-8b7b-1bd3ebc7bdee"
   },
   "source": [
    "## Demo\n",
    "\n",
    "Let's check this system on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b1c468e-c86c-46a7-b2d7-f2dd15f1b3cf",
    "_uuid": "de3b663f-f476-4001-b384-eeb4ca01c631"
   },
   "outputs": [],
   "source": [
    "autocorrect(\"he is goinng home\", vocab, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "13f9bb6c-8f64-4ded-8797-bc66e199e6d6",
    "_uuid": "465fc0f4-2b61-4599-afcb-426892e57c2c"
   },
   "outputs": [],
   "source": [
    "autocorrect(\"honsty is the best pooliccy\", vocab, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d83356e7-1e93-43ec-8f62-41b2d5ea5e5f",
    "_uuid": "33daf857-6695-463f-be49-e5b3549dde04"
   },
   "outputs": [],
   "source": [
    "autocorrect(\"life is a diink annd lve is a druug\", vocab, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "40e59d8d-833b-447a-90ef-5e0558390485",
    "_uuid": "c242318b-e269-435f-bdcc-8ae7d50c3863"
   },
   "source": [
    "### We can see that it is working!\n",
    "\n",
    "This gives a overview of what auto-correct systems are and how they work.\n",
    "\n",
    "Note - This is very simplified architecture compared to what is used in reality. You can see in the last example's output isn't good. It is supposed to be (\"Life is a drink and love is a drug\")\n",
    "\n",
    "#### Drawbacks \n",
    "- It has fixed outcome. i.e. 'hime' will be converted to 'time' only not 'home' oe anything else.\n",
    "- It is solely based on frequency of words in the corpus\n",
    "- Doesn't care about the contex.\n",
    "- Can't suggest something which is not in the vocabulary\n",
    "\n",
    "#### Improvements\n",
    "- It can be further improved by introducing bi-gram probabilities. Hence, it will get some inference from previous words.\n",
    "- The suggestions that are less distance away from the misspelled word are more likely. Hence, the system can be further improved by introducing dynamic programming based min edit distance functionality.\n",
    "\n",
    "Let's implement these improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Improvement 1 : Introducing n-gram probabilities to get context from previous words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea is taken from the n-grams language models. In a n-gram language model\n",
    "- Assume the probability of the next word depends only on the previous n-gram.\n",
    "- The previous n-gram is the series of the previous 'n' words.\n",
    "\n",
    "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ is:\n",
    "\n",
    "$$ P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}$$\n",
    "\n",
    "This probability cab be estimated by counting the occurrences of these series of words in the training data.\n",
    "- The probability can be estimated as a ratio, where\n",
    "- The numerator is the number of times word 't' appears after words t-1 through t-n appear in the training data.\n",
    "- The denominator is the number of times word t-1 through t-n appears in the training data.\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
    "\n",
    "In other words, to estimate probabilities based on n-grams, first find the counts of n-grams (for denominator) then divide it by the count of (n+1)-grams (for numerator).\n",
    "\n",
    "- The function $C(\\cdots)$ denotes the number of occurence of the given sequence. \n",
    "- $\\hat{P}$ means the estimation of $P$. \n",
    "- The denominator of the above equation is the number of occurence of the previous $n$ words, and the numerator is the same sequence followed by the word $w_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the issue with above formula is that it doesn't work when a count of an n-gram is zero..\n",
    "- Suppose we encounter an n-gram that did not occur in the training data.  \n",
    "- Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).\n",
    "\n",
    "A way to handle zero counts is to add k-smoothing.  \n",
    "- K-smoothing adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "\n",
    "For n-grams that have a zero count, the equation (3) becomes $\\frac{1}{|V|}$.\n",
    "- This means that any n-gram with zero count has the same probability of $\\frac{1}{|V|}$.\n",
    "\n",
    "Now, let's define a function that computes the probability estimate (3) from n-gram counts and a constant $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "\n",
    "    \n",
    "    for sentence in data: \n",
    "        \n",
    "        # prepend start token n times, and  append <e> one time\n",
    "        sentence = [start_token]*n + sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        for i in range(len(sentence)-n): \n",
    "            n_gram = sentence[i:i+n]\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOME UTILITY\n",
    "\n",
    "def split_to_sentences(data):\n",
    "    #sentences = data.split(\"\\n\")\n",
    "    sentences = [s.strip() for s in data]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences    \n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = nltk.tokenize.word_tokenize(sentence)\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def get_tokenized_data(data):\n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(file)\n",
    "bigram_counts = count_n_grams(tokenized_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_prob(word, prev_word, bigram_counts, factor):\n",
    "    key = tuple([prev_word, word])\n",
    "    #print(key)  \n",
    "    \n",
    "    ksum = 0\n",
    "    occ = 0\n",
    "    for k, v in bigram_counts.items():\n",
    "        if k[0] == prev_word:\n",
    "            ksum = ksum + v\n",
    "            occ = occ + 1\n",
    "    #print(ksum)\n",
    "    #print(occ)\n",
    "    \n",
    "    count = 0\n",
    "    if key in bigram_counts.keys():\n",
    "        count = bigram_counts[key]\n",
    "    #print(type(occ))\n",
    "    \n",
    "    smooth_count = count + factor \n",
    "    smooth_occ = ksum + occ*factor\n",
    "    probability = smooth_count / smooth_occ\n",
    "    #print(probability)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bigram_prob('is', 'that', bigram_counts, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the bigram probabilities now we'll update our get_correction function to incorporate the changes.\n",
    "\n",
    "We won't be ignoring the unigram probabilities completely instead we will assing weights to unigram and bigram. i.e. \n",
    "\n",
    "> final_score = unigram_weight*unigram_prob + bigram_weight*bigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections_bigram(word, prev_word, probs, vocab, bigram_counts, unigram_weight=0.3, bigram_weight=0.7, n=5, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "   \n",
    "    if word in probs.keys():\n",
    "        suggestions.append(word)\n",
    "    for w in edit_one_letter(word):\n",
    "        if len(suggestions) == n:\n",
    "            break\n",
    "        if w in probs.keys():\n",
    "            suggestions.append(w)\n",
    "    for w in edit_two_letters(word):\n",
    "        if len(suggestions) == n:\n",
    "            break\n",
    "        if w in probs.keys():\n",
    "            suggestions.append(w)\n",
    "        \n",
    "        \n",
    "    best_words = {}\n",
    "    \n",
    "    for s in suggestions:\n",
    "        #best_words[s] = probs[s]\n",
    "        unigram_prob = probs[s]\n",
    "        #print(s)\n",
    "        try:\n",
    "            bigram_prob = get_bigram_prob(s, prev_word, bigram_counts, 1)\n",
    "        except:\n",
    "            bigram_prob = 0.0000000000000000001\n",
    "\n",
    "        final_score = unigram_weight*unigram_prob + bigram_weight*bigram_prob\n",
    "        \n",
    "        best_words[s] = final_score     \n",
    "        \n",
    "    best_words = sorted(best_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    n_best = best_words \n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_word_bigram(word, prev_word, probs, vocab, bigram_counts, unigram_weight, bigram_weight, n): \n",
    "    corrections = get_corrections_bigram(word, prev_word, probs, vocab, \n",
    "                                         bigram_counts, unigram_weight, bigram_weight, n, verbose=False)\n",
    "    #print(corrections)\n",
    "    if len(corrections) == 0:\n",
    "        return word\n",
    "    \n",
    "    final_word = corrections[0][0]\n",
    "    final_prob = corrections[0][1]\n",
    "    for i, word_prob in enumerate(corrections):\n",
    "        #print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "        if word_prob[1] > final_prob:\n",
    "            final_word = word_prob[0]\n",
    "            final_prob = word_prob[1]\n",
    "    return final_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect_bigram(sentence, vocab, probs, bigram_counts):\n",
    "    print(\"Input sentence : \", sentence)\n",
    "    wrong_words = find_wrong_word(sentence, vocab)\n",
    "    print(\"Wrong words : \", wrong_words)\n",
    "    #print(wrong_words)\n",
    "    correct_words = []\n",
    "    word_list = sentence.strip().lower().split(\" \")\n",
    "    for i, word in enumerate(word_list):\n",
    "        #print(i, word)\n",
    "        \n",
    "        #### Previous word\n",
    "        if i==0:\n",
    "            prev_word = '<s>'\n",
    "        else:\n",
    "            prev_word = word_list[i-1]\n",
    "            \n",
    "        if word in wrong_words:\n",
    "            correct_word = get_correct_word_bigram(word, prev_word, probs, vocab, bigram_counts, 0.3, 0.7, 10)\n",
    "            #print(word, correct_word)\n",
    "            word = correct_word\n",
    "        correct_words.append(word)\n",
    "    print(\"Output Sentence : \", \" \".join(correct_words).capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrect_bigram('she is really beutifule', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrect_bigram('you are not alowwed here', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrect_bigram('physics is the most amainzg subect', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is impressive! Now let's implement the next improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Improvement 2 : Introducing min_edit_diatsnce functionality\n",
    "\n",
    "The idea is derived from the intution that the suggestions that are less distance away from the misspelled word are more likely. Hence, the system can be further improved by introducing dynamic programming based min edit distance functionality.\n",
    "\n",
    "So, given a string source[0..i] and a string target[0..j], we will compute all the combinations of substrings[i, j] and calculate their edit distance. To do this efficiently, we will use a table to maintain the previously computed substrings and use those to calculate larger substrings.\n",
    "\n",
    "We'll first create a matrix and update each element in the matrix as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Initialization}$$\n",
    "\n",
    "\\begin{align}\n",
    "D[0,0] &= 0 \\\\\n",
    "D[i,0] &= D[i-1,0] + del\\_cost(source[i]) \\\\\n",
    "D[0,j] &= D[0,j-1] + ins\\_cost(target[j]) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Per Cell Operations}$$\n",
    "\\begin{align}\n",
    " \\\\\n",
    "D[i,j] =min\n",
    "\\begin{cases}\n",
    "D[i-1,j] + del\\_cost\\\\\n",
    "D[i,j-1] + ins\\_cost\\\\\n",
    "D[i-1,j-1] + \\left\\{\\begin{matrix}\n",
    "rep\\_cost; & if src[i]\\neq tar[j]\\\\\n",
    "0 ; & if src[i]=tar[j]\n",
    "\\end{matrix}\\right.\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    \n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    for row in range(1,m+1): \n",
    "        D[row,0] = D[row-1, 0] + del_cost\n",
    "        \n",
    "    for col in range(1,n+1):\n",
    "        D[0,col] = D[0, col-1] + ins_cost\n",
    "        \n",
    "    # Loop through row 1 to row m\n",
    "    for row in range(1,m+1): \n",
    "        # Loop through column 1 to column n\n",
    "        for col in range(1,n+1):\n",
    "            # Intialize r_cost to the 'replace' cost \n",
    "            r_cost = rep_cost\n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            D[row,col] = D[row-1][col-1] + r_cost\n",
    "          \n",
    "    # Set the minimum edit distance with the cost found at row m, column n\n",
    "    med = D[m][n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_word_bigram_min_edit(word, prev_word, probs, vocab, bigram_counts, unigram_weight, bigram_weight, n, scale_dist): \n",
    "    corrections = get_corrections_bigram(word, prev_word, probs, vocab, \n",
    "                                         bigram_counts, unigram_weight, bigram_weight, n, verbose=False)\n",
    "    #print(corrections)\n",
    "    if len(corrections) == 0:\n",
    "        return word\n",
    "    \n",
    "    ### Make a dataframe of suggestions\n",
    "    words = []\n",
    "    probabs = []\n",
    "    dist = []\n",
    "    for pair in corrections:\n",
    "        words.append(pair[0])\n",
    "        probabs.append(pair[1])\n",
    "        _, distance = min_edit_distance(word, pair[0], 1, 1, 2)\n",
    "        dist.append(distance)\n",
    "        \n",
    "    df = pd.DataFrame({'suggestion':words, 'distance':dist, 'probability':probabs})\n",
    "    df['inv_dist'] = df['distance'].apply(lambda x : (1/x)*scale_dist)\n",
    "    df['score'] = df['inv_dist'] + df['probability']\n",
    "    df = df.sort_values(by='score', ascending=False)\n",
    "    #df = df.sort_values(by=['distance', 'probability'], ascending=[True, False])\n",
    "    #display(df)\n",
    "    \n",
    "    final_word = df.iloc[0,0]\n",
    "    \n",
    "    return final_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correct_word_bigram_min_edit('pulicy', 'best', probs, vocab, bigram_counts, 0.3, 0.7, 10, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect_bigram_min_edit(sentence, vocab, probs, bigram_probability_df, scale_dist=0.001):\n",
    "    print(\"Input sentence : \", sentence)\n",
    "    wrong_words = find_wrong_word(sentence, vocab)\n",
    "    print(\"Wrong words : \", wrong_words)\n",
    "    #print(wrong_words)\n",
    "    correct_words = []\n",
    "    word_list = sentence.strip().lower().split(\" \")\n",
    "    for i, word in enumerate(word_list):\n",
    "        #print(i, word)\n",
    "        \n",
    "        #### Previous word\n",
    "        if i==0:\n",
    "            prev_word = '<s>'\n",
    "        else:\n",
    "            prev_word = word_list[i-1]\n",
    "            \n",
    "        if word in wrong_words:\n",
    "            correct_word = get_correct_word_bigram_min_edit(word, prev_word, probs, vocab, bigram_probability_df, 0.3, 0.7, 25, scale_dist)\n",
    "            #print(word, correct_word)\n",
    "            word = correct_word\n",
    "        correct_words.append(word)\n",
    "    print(\"Output Sentence : \", \" \".join(correct_words).capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrect_bigram_min_edit('I have acess to the lidrary', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrect_bigram_min_edit('presnet for the meeating', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrect_bigram_min_edit('he planed a game', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's design a unit test for the auto-correct system that we have develpoed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests1 = {'access': 'acess',\n",
    "          'accessing': 'accesing',\n",
    "          'accommodation': 'accomodation acommodation acomodation',\n",
    "          'account': 'acount',\n",
    "          'address': 'adress adres',\n",
    "          'addressable': 'addresable',\n",
    "          'arranged': 'aranged arrainged',\n",
    "          'arrangeing': 'aranging',\n",
    "          'arrangement': 'arragment',\n",
    "          'articles': 'articals',\n",
    "          'aunt': 'annt anut arnt',\n",
    "          'auxiliary': 'auxillary',\n",
    "          'available': 'avaible',\n",
    "          'awful': 'awfall afful',\n",
    "          'basically': 'basicaly',\n",
    "          'beginning': 'begining',\n",
    "          'benefit': 'benifit',\n",
    "          'benefits': 'benifits',\n",
    "          'between': 'beetween',\n",
    "          'bicycle': 'bicycal bycicle bycycle',\n",
    "          'biscuits': 'biscits biscutes biscuts bisquits buiscits buiscuts',\n",
    "          'built': 'biult',\n",
    "          'cake': 'cak',\n",
    "          'career': 'carrer',\n",
    "          'cemetery': 'cemetary semetary',\n",
    "          'centrally': 'centraly',\n",
    "          'certain': 'cirtain',\n",
    "          'challenges': 'chalenges chalenges',\n",
    "          'chapter': 'chaper chaphter chaptur',\n",
    "          'choice': 'choise',\n",
    "          'choosing': 'chosing',\n",
    "          'clerical': 'clearical',\n",
    "          'committee': 'comittee',\n",
    "          'compare': 'compair',\n",
    "          'completely': 'completly',\n",
    "          'consider': 'concider',\n",
    "          'considerable': 'conciderable',\n",
    "          'contented': 'contenpted contende contended contentid',\n",
    "          'curtains': 'cartains certans courtens cuaritains curtans curtians curtions',\n",
    "          'decide': 'descide',\n",
    "          'decided': 'descided',\n",
    "          'definitely': 'definately difinately',\n",
    "          'definition': 'defenition',\n",
    "          'definitions': 'defenitions',\n",
    "          'description': 'discription',\n",
    "          'desiccate': 'desicate dessicate dessiccate',\n",
    "          'diagrammatically': 'diagrammaticaally',\n",
    "          'different': 'diffrent',\n",
    "          'driven': 'dirven',\n",
    "          'ecstasy': 'exstacy ecstacy',\n",
    "          'embarrass': 'embaras embarass',\n",
    "          'establishing': 'astablishing establising',\n",
    "          'experience': 'experance experiance',\n",
    "          'experiences': 'experances',\n",
    "          'extended': 'extented',\n",
    "          'extremely': 'extreamly',\n",
    "          'fails': 'failes',\n",
    "          'families': 'familes',\n",
    "          'february': 'febuary',\n",
    "          'further': 'futher',\n",
    "          'gallery': 'galery gallary gallerry gallrey',\n",
    "          'hierarchal': 'hierachial',\n",
    "          'hierarchy': 'hierchy',\n",
    "          'inconvenient': 'inconvienient inconvient inconvinient',\n",
    "          'independent': 'independant independant',\n",
    "          'initial': 'intial',\n",
    "          'initials': 'inetials inistals initails initals intials',\n",
    "          'juice': 'guic juce jucie juise juse',\n",
    "          'latest': 'lates latets latiest latist',\n",
    "          'laugh': 'lagh lauf laught lugh',\n",
    "          'level': 'leval',\n",
    "          'levels': 'levals',\n",
    "          'liaison': 'liaision liason',\n",
    "          'lieu': 'liew',\n",
    "          'literature': 'litriture',\n",
    "          'loans': 'lones',\n",
    "          'locally': 'localy',\n",
    "          'magnificent': 'magnificnet magificent magnifcent magnifecent magnifiscant magnifisent magnificant',\n",
    "          'management': 'managment',\n",
    "          'meant': 'ment',\n",
    "          'minuscule': 'miniscule',\n",
    "          'minutes': 'muinets',\n",
    "          'monitoring': 'monitering',\n",
    "          'necessary': 'neccesary necesary neccesary necassary necassery neccasary',\n",
    "          'occurrence': 'occurence occurence',\n",
    "          'often': 'ofen offen offten ofton',\n",
    "          'opposite': 'opisite oppasite oppesite oppisit oppisite opposit oppossite oppossitte',\n",
    "          'parallel': 'paralel paralell parrallel parralell parrallell',\n",
    "          'particular': 'particulaur',\n",
    "          'perhaps': 'perhapse',\n",
    "          'personnel': 'personnell',\n",
    "          'planned': 'planed',\n",
    "          'poem': 'poame',\n",
    "          'poems': 'poims pomes',\n",
    "          'poetry': 'poartry poertry poetre poety powetry',\n",
    "          'position': 'possition',\n",
    "          'possible': 'possable',\n",
    "          'pretend': 'pertend protend prtend pritend',\n",
    "          'problem': 'problam proble promblem proplen',\n",
    "          'pronunciation': 'pronounciation',\n",
    "          'purple': 'perple perpul poarple',\n",
    "          'questionnaire': 'questionaire',\n",
    "          'really': 'realy relley relly',\n",
    "          'receipt': 'receit receite reciet recipt',\n",
    "          'receive': 'recieve',\n",
    "          'refreshment': 'reafreshment refreshmant refresment refressmunt',\n",
    "          'remember': 'rember remeber rememmer rermember',\n",
    "          'remind': 'remine remined',\n",
    "          'scarcely': 'scarcly scarecly scarely scarsely',\n",
    "          'scissors': 'scisors sissors',\n",
    "          'separate': 'seperate',\n",
    "          'singular': 'singulaur',\n",
    "          'someone': 'somone',\n",
    "          'sources': 'sorces',\n",
    "          'southern': 'southen',\n",
    "          'special': 'speaical specail specal speical',\n",
    "          'splendid': 'spledid splended splened splended',\n",
    "          'standardizing': 'stanerdizing',\n",
    "          'stomach': 'stomac stomache stomec stumache',\n",
    "          'supersede': 'supercede superceed',\n",
    "          'there': 'ther',\n",
    "          'totally': 'totaly',\n",
    "          'transferred': 'transfred',\n",
    "          'transportability': 'transportibility',\n",
    "          'triangular': 'triangulaur',\n",
    "          'understand': 'undersand undistand',\n",
    "          'unexpected': 'unexpcted unexpeted unexspected',\n",
    "          'unfortunately': 'unfortunatly',\n",
    "          'unique': 'uneque',\n",
    "          'useful': 'usefull',\n",
    "          'valuable': 'valubale valuble',\n",
    "          'variable': 'varable',\n",
    "          'variant': 'vairiant',\n",
    "          'various': 'vairious',\n",
    "          'visited': 'fisited viseted vistid vistied',\n",
    "          'visitors': 'vistors',\n",
    "          'voluntary': 'volantry',\n",
    "          'voting': 'voteing',\n",
    "          'wanted': 'wantid wonted',\n",
    "          'whether': 'wether',\n",
    "          'wrote': 'rote wote'}\n",
    "\n",
    "tests2 = {'forbidden': 'forbiden',\n",
    "          'decisions': 'deciscions descisions',\n",
    "          'supposedly': 'supposidly',\n",
    "          'embellishing': 'embelishing',\n",
    "          'technique': 'tecnique',\n",
    "          'permanently': 'perminantly',\n",
    "          'confirmation': 'confermation',\n",
    "          'appointment': 'appoitment',\n",
    "          'progression': 'progresion',\n",
    "          'accompanying': 'acompaning',\n",
    "          'applicable': 'aplicable',\n",
    "          'regained': 'regined',\n",
    "          'guidelines': 'guidlines',\n",
    "          'surrounding': 'serounding',\n",
    "          'titles': 'tittles',\n",
    "          'unavailable': 'unavailble',\n",
    "          'advantageous': 'advantageos',\n",
    "          'brief': 'brif',\n",
    "          'appeal': 'apeal',\n",
    "          'consisting': 'consisiting',\n",
    "          'clerk': 'cleark clerck',\n",
    "          'component': 'componant',\n",
    "          'favourable': 'faverable',\n",
    "          'separation': 'seperation',\n",
    "          'search': 'serch',\n",
    "          'receive': 'recieve',\n",
    "          'employees': 'emploies',\n",
    "          'prior': 'piror',\n",
    "          'resulting': 'reulting',\n",
    "          'suggestion': 'sugestion',\n",
    "          'opinion': 'oppinion',\n",
    "          'cancellation': 'cancelation',\n",
    "          'criticism': 'citisum',\n",
    "          'useful': 'usful',\n",
    "          'humour': 'humor',\n",
    "          'anomalies': 'anomolies',\n",
    "          'would': 'whould',\n",
    "          'doubt': 'doupt',\n",
    "          'examination': 'eximination',\n",
    "          'therefore': 'therefoe',\n",
    "          'recommend': 'recomend',\n",
    "          'separated': 'seperated',\n",
    "          'successful': 'sucssuful succesful',\n",
    "          'apparent': 'apparant',\n",
    "          'occurred': 'occureed',\n",
    "          'particular': 'paerticulaur',\n",
    "          'pivoting': 'pivting',\n",
    "          'announcing': 'anouncing',\n",
    "          'challenge': 'chalange',\n",
    "          'arrangements': 'araingements',\n",
    "          'proportions': 'proprtions',\n",
    "          'organized': 'oranised',\n",
    "          'accept': 'acept',\n",
    "          'dependence': 'dependance',\n",
    "          'unequalled': 'unequaled',\n",
    "          'numbers': 'numbuers',\n",
    "          'sense': 'sence',\n",
    "          'conversely': 'conversly',\n",
    "          'provide': 'provid',\n",
    "          'arrangement': 'arrangment',\n",
    "          'responsibilities': 'responsiblities',\n",
    "          'fourth': 'forth',\n",
    "          'ordinary': 'ordenary',\n",
    "          'description': 'desription descvription desacription',\n",
    "          'inconceivable': 'inconcievable',\n",
    "          'data': 'dsata',\n",
    "          'register': 'rgister',\n",
    "          'supervision': 'supervison',\n",
    "          'encompassing': 'encompasing',\n",
    "          'negligible': 'negligable',\n",
    "          'allow': 'alow',\n",
    "          'operations': 'operatins',\n",
    "          'executed': 'executted',\n",
    "          'interpretation': 'interpritation',\n",
    "          'hierarchy': 'heiarky',\n",
    "          'indeed': 'indead',\n",
    "          'years': 'yesars',\n",
    "          'through': 'throut',\n",
    "          'committee': 'committe',\n",
    "          'inquiries': 'equiries',\n",
    "          'before': 'befor',\n",
    "          'continued': 'contuned',\n",
    "          'permanent': 'perminant',\n",
    "          'choose': 'chose',\n",
    "          'virtually': 'vertually',\n",
    "          'correspondence': 'correspondance',\n",
    "          'eventually': 'eventully',\n",
    "          'lonely': 'lonley',\n",
    "          'profession': 'preffeson',\n",
    "          'they': 'thay',\n",
    "          'now': 'noe',\n",
    "          'desperately': 'despratly',\n",
    "          'university': 'unversity',\n",
    "          'adjournment': 'adjurnment',\n",
    "          'possibilities': 'possablities',\n",
    "          'stopped': 'stoped',\n",
    "          'mean': 'meen',\n",
    "          'weighted': 'wagted',\n",
    "          'adequately': 'adequattly',\n",
    "          'shown': 'hown',\n",
    "          'matrix': 'matriiix',\n",
    "          'profit': 'proffit',\n",
    "          'encourage': 'encorage',\n",
    "          'collate': 'colate',\n",
    "          'disaggregate': 'disaggreagte disaggreaget',\n",
    "          'receiving': 'recieving reciving',\n",
    "          'proviso': 'provisoe',\n",
    "          'umbrella': 'umberalla',\n",
    "          'approached': 'aproached',\n",
    "          'pleasant': 'plesent',\n",
    "          'difficulty': 'dificulty',\n",
    "          'appointments': 'apointments',\n",
    "          'base': 'basse',\n",
    "          'conditioning': 'conditining',\n",
    "          'earliest': 'earlyest',\n",
    "          'beginning': 'begining',\n",
    "          'universally': 'universaly',\n",
    "          'unresolved': 'unresloved',\n",
    "          'length': 'lengh',\n",
    "          'exponentially': 'exponentualy',\n",
    "          'utilized': 'utalised',\n",
    "          'set': 'et',\n",
    "          'surveys': 'servays',\n",
    "          'families': 'familys',\n",
    "          'system': 'sysem',\n",
    "          'approximately': 'aproximatly',\n",
    "          'their': 'ther',\n",
    "          'scheme': 'scheem',\n",
    "          'speaking': 'speeking',\n",
    "          'repetitive': 'repetative',\n",
    "          'inefficient': 'ineffiect',\n",
    "          'geneva': 'geniva',\n",
    "          'exactly': 'exsactly',\n",
    "          'immediate': 'imediate',\n",
    "          'appreciation': 'apreciation',\n",
    "          'luckily': 'luckeley',\n",
    "          'eliminated': 'elimiated',\n",
    "          'believe': 'belive',\n",
    "          'appreciated': 'apreciated',\n",
    "          'readjusted': 'reajusted',\n",
    "          'were': 'wer where',\n",
    "          'feeling': 'fealing',\n",
    "          'and': 'anf',\n",
    "          'false': 'faulse',\n",
    "          'seen': 'seeen',\n",
    "          'interrogating': 'interogationg',\n",
    "          'academically': 'academicly',\n",
    "          'relatively': 'relativly relitivly',\n",
    "          'traditionally': 'traditionaly',\n",
    "          'studying': 'studing',\n",
    "          'majority': 'majorty',\n",
    "          'build': 'biuld',\n",
    "          'aggravating': 'agravating',\n",
    "          'transactions': 'trasactions',\n",
    "          'arguing': 'aurguing',\n",
    "          'sheets': 'sheertes',\n",
    "          'successive': 'sucsesive sucessive',\n",
    "          'segment': 'segemnt',\n",
    "          'especially': 'especaily',\n",
    "          'later': 'latter',\n",
    "          'senior': 'sienior',\n",
    "          'dragged': 'draged',\n",
    "          'atmosphere': 'atmospher',\n",
    "          'drastically': 'drasticaly',\n",
    "          'particularly': 'particulary',\n",
    "          'visitor': 'vistor',\n",
    "          'session': 'sesion',\n",
    "          'continually': 'contually',\n",
    "          'availability': 'avaiblity',\n",
    "          'busy': 'buisy',\n",
    "          'parameters': 'perametres',\n",
    "          'surroundings': 'suroundings seroundings',\n",
    "          'employed': 'emploied',\n",
    "          'adequate': 'adiquate',\n",
    "          'handle': 'handel',\n",
    "          'means': 'meens',\n",
    "          'familiar': 'familer',\n",
    "          'between': 'beeteen',\n",
    "          'overall': 'overal',\n",
    "          'timing': 'timeing',\n",
    "          'committees': 'comittees commitees',\n",
    "          'queries': 'quies',\n",
    "          'econometric': 'economtric',\n",
    "          'erroneous': 'errounous',\n",
    "          'decides': 'descides',\n",
    "          'reference': 'refereence refference',\n",
    "          'intelligence': 'inteligence',\n",
    "          'edition': 'ediion ediition',\n",
    "          'are': 'arte',\n",
    "          'apologies': 'appologies',\n",
    "          'thermawear': 'thermawere thermawhere',\n",
    "          'techniques': 'tecniques',\n",
    "          'voluntary': 'volantary',\n",
    "          'subsequent': 'subsequant subsiquent',\n",
    "          'currently': 'curruntly',\n",
    "          'forecast': 'forcast',\n",
    "          'weapons': 'wepons',\n",
    "          'routine': 'rouint',\n",
    "          'neither': 'niether',\n",
    "          'approach': 'aproach',\n",
    "          'available': 'availble',\n",
    "          'recently': 'reciently',\n",
    "          'ability': 'ablity',\n",
    "          'nature': 'natior',\n",
    "          'commercial': 'comersial',\n",
    "          'agencies': 'agences',\n",
    "          'however': 'howeverr',\n",
    "          'suggested': 'sugested',\n",
    "          'career': 'carear',\n",
    "          'many': 'mony',\n",
    "          'annual': 'anual',\n",
    "          'according': 'acording',\n",
    "          'receives': 'recives recieves',\n",
    "          'interesting': 'intresting',\n",
    "          'expense': 'expence',\n",
    "          'relevant': 'relavent relevaant',\n",
    "          'table': 'tasble',\n",
    "          'throughout': 'throuout',\n",
    "          'conference': 'conferance',\n",
    "          'sensible': 'sensable',\n",
    "          'described': 'discribed describd',\n",
    "          'union': 'unioun',\n",
    "          'interest': 'intrest',\n",
    "          'flexible': 'flexable',\n",
    "          'refered': 'reffered',\n",
    "          'controlled': 'controled',\n",
    "          'sufficient': 'suficient',\n",
    "          'dissension': 'desention',\n",
    "          'adaptable': 'adabtable',\n",
    "          'representative': 'representitive',\n",
    "          'irrelevant': 'irrelavent',\n",
    "          'unnecessarily': 'unessasarily',\n",
    "          'applied': 'upplied',\n",
    "          'apologised': 'appologised',\n",
    "          'these': 'thees thess',\n",
    "          'choices': 'choises',\n",
    "          'will': 'wil',\n",
    "          'procedure': 'proceduer',\n",
    "          'shortened': 'shortend',\n",
    "          'manually': 'manualy',\n",
    "          'disappointing': 'dissapoiting',\n",
    "          'excessively': 'exessively',\n",
    "          'comments': 'coments',\n",
    "          'containing': 'containg',\n",
    "          'develop': 'develope',\n",
    "          'credit': 'creadit',\n",
    "          'government': 'goverment',\n",
    "          'acquaintances': 'aquantences',\n",
    "          'orientated': 'orentated',\n",
    "          'widely': 'widly',\n",
    "          'advise': 'advice',\n",
    "          'difficult': 'dificult',\n",
    "          'investigated': 'investegated',\n",
    "          'bonus': 'bonas',\n",
    "          'conceived': 'concieved',\n",
    "          'nationally': 'nationaly',\n",
    "          'compared': 'comppared compased',\n",
    "          'moving': 'moveing',\n",
    "          'necessity': 'nessesity',\n",
    "          'opportunity': 'oppertunity oppotunity opperttunity',\n",
    "          'thoughts': 'thorts',\n",
    "          'equalled': 'equaled',\n",
    "          'variety': 'variatry',\n",
    "          'analysis': 'analiss analsis analisis',\n",
    "          'patterns': 'pattarns',\n",
    "          'qualities': 'quaties',\n",
    "          'easily': 'easyly',\n",
    "          'organization': 'oranisation oragnisation',\n",
    "          'the': 'thw hte thi',\n",
    "          'corporate': 'corparate',\n",
    "          'composed': 'compossed',\n",
    "          'enormously': 'enomosly',\n",
    "          'financially': 'financialy',\n",
    "          'functionally': 'functionaly',\n",
    "          'discipline': 'disiplin',\n",
    "          'announcement': 'anouncement',\n",
    "          'progresses': 'progressess',\n",
    "          'except': 'excxept',\n",
    "          'recommending': 'recomending',\n",
    "          'mathematically': 'mathematicaly',\n",
    "          'source': 'sorce',\n",
    "          'combine': 'comibine',\n",
    "          'input': 'inut',\n",
    "          'careers': 'currers carrers',\n",
    "          'resolved': 'resoved',\n",
    "          'demands': 'diemands',\n",
    "          'unequivocally': 'unequivocaly',\n",
    "          'suffering': 'suufering',\n",
    "          'immediately': 'imidatly imediatly',\n",
    "          'accepted': 'acepted',\n",
    "          'projects': 'projeccts',\n",
    "          'necessary': 'necasery nessasary nessisary neccassary',\n",
    "          'journalism': 'journaism',\n",
    "          'unnecessary': 'unessessay',\n",
    "          'night': 'nite',\n",
    "          'output': 'oputput',\n",
    "          'security': 'seurity',\n",
    "          'essential': 'esential',\n",
    "          'beneficial': 'benificial benficial',\n",
    "          'explaining': 'explaning',\n",
    "          'supplementary': 'suplementary',\n",
    "          'questionnaire': 'questionare',\n",
    "          'employment': 'empolyment',\n",
    "          'proceeding': 'proceding',\n",
    "          'decision': 'descisions descision',\n",
    "          'per': 'pere',\n",
    "          'discretion': 'discresion',\n",
    "          'reaching': 'reching',\n",
    "          'analysed': 'analised',\n",
    "          'expansion': 'expanion',\n",
    "          'although': 'athough',\n",
    "          'subtract': 'subtrcat',\n",
    "          'analysing': 'aalysing',\n",
    "          'comparison': 'comparrison',\n",
    "          'months': 'monthes',\n",
    "          'hierarchal': 'hierachial',\n",
    "          'misleading': 'missleading',\n",
    "          'commit': 'comit',\n",
    "          'auguments': 'aurgument',\n",
    "          'within': 'withing',\n",
    "          'obtaining': 'optaning',\n",
    "          'accounts': 'acounts',\n",
    "          'primarily': 'pimarily',\n",
    "          'operator': 'opertor',\n",
    "          'accumulated': 'acumulated',\n",
    "          'extremely': 'extreemly',\n",
    "          'there': 'thear',\n",
    "          'summarys': 'sumarys',\n",
    "          'analyse': 'analiss',\n",
    "          'understandable': 'understadable',\n",
    "          'safeguard': 'safegaurd',\n",
    "          'consist': 'consisit',\n",
    "          'declarations': 'declaratrions',\n",
    "          'minutes': 'muinutes muiuets',\n",
    "          'associated': 'assosiated',\n",
    "          'accessibility': 'accessability',\n",
    "          'examine': 'examin',\n",
    "          'surveying': 'servaying',\n",
    "          'politics': 'polatics',\n",
    "          'annoying': 'anoying',\n",
    "          'again': 'agiin',\n",
    "          'assessing': 'accesing',\n",
    "          'ideally': 'idealy',\n",
    "          'scrutinized': 'scrutiniesed',\n",
    "          'simular': 'similar',\n",
    "          'personnel': 'personel',\n",
    "          'whereas': 'wheras',\n",
    "          'when': 'whn',\n",
    "          'geographically': 'goegraphicaly',\n",
    "          'gaining': 'ganing',\n",
    "          'requested': 'rquested',\n",
    "          'separate': 'seporate',\n",
    "          'students': 'studens',\n",
    "          'prepared': 'prepaired',\n",
    "          'generated': 'generataed',\n",
    "          'graphically': 'graphicaly',\n",
    "          'suited': 'suted',\n",
    "          'variable': 'varible vaiable',\n",
    "          'building': 'biulding',\n",
    "          'required': 'reequired',\n",
    "          'necessitates': 'nessisitates',\n",
    "          'together': 'togehter',\n",
    "          'profits': 'proffits'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autocorrect(utdata, vocab, probs, string):\n",
    "    tcount = 0\n",
    "    fcount = 0\n",
    "    rcount = 0\n",
    "    print(\"Running \"+string+\" : Basic Auto-correct system\")\n",
    "    for k, v in utdata.items():\n",
    "        incorrect_list = v.strip().split()\n",
    "        #print(incorrect_list)\n",
    "        for w in incorrect_list:\n",
    "            tcount = tcount + 1\n",
    "            cw = get_correct_word(w, vocab, probs, 25)\n",
    "            if cw==k:\n",
    "                #print('correct')\n",
    "                rcount = rcount + 1\n",
    "            else:\n",
    "                #print('wrong')\n",
    "                fcount = fcount + 1\n",
    "    print(\"Accuracy : {} %\".format((rcount/tcount)*100))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autocorrect_bigram(utdata, vocab, probs, string, bigram_counts):\n",
    "    tcount = 0\n",
    "    fcount = 0\n",
    "    rcount = 0\n",
    "    print(\"Running \"+string+\" : Bi-gram Auto-correct system\")\n",
    "    for k, v in utdata.items():\n",
    "        incorrect_list = v.strip().split()\n",
    "        #print(incorrect_list)\n",
    "        for w in incorrect_list:\n",
    "            tcount = tcount + 1\n",
    "            cw = get_correct_word_bigram(w, '<s>', probs, vocab, bigram_counts, 0.3, 0.7, 25)\n",
    "            if cw==k:\n",
    "                #print('correct')\n",
    "                rcount = rcount + 1\n",
    "            else:\n",
    "                #print('wrong')\n",
    "                fcount = fcount + 1\n",
    "    print(\"Accuracy : {} %\".format((rcount/tcount)*100))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autocorrect_bigram_min_edit(utdata, vocab, probs, string, bigram_counts):\n",
    "    tcount = 0\n",
    "    fcount = 0\n",
    "    rcount = 0\n",
    "    print(\"Running \"+string+\" : Bi-gram (min-edit) Auto-correct system\")\n",
    "    for k, v in utdata.items():\n",
    "        incorrect_list = v.strip().split()\n",
    "        #print(incorrect_list)\n",
    "        for w in incorrect_list:\n",
    "            tcount = tcount + 1\n",
    "            cw = get_correct_word_bigram_min_edit(w, '<s>', probs, vocab, bigram_counts, 0.3, 0.7, 25, 0.000001)\n",
    "            if cw==k:\n",
    "                #print('correct')\n",
    "                rcount = rcount + 1\n",
    "            else:\n",
    "                #print('wrong')\n",
    "                fcount = fcount + 1\n",
    "    print(\"Accuracy : {} %\".format((rcount/tcount)*100))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic auto-correct\n",
    "test_autocorrect(tests1, vocab, probs, \"Unit Test 1\")\n",
    "test_autocorrect(tests2, vocab, probs, \"Unit Test 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tests aren't very suitable for bigram architecture because it uses previous word. We'll can pass start token as previous word to test the bigram based auto-correct. Same is the case with min-edit architecture. {currectly commnted the testing part to reduce the run-time of kaggle notebook.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_autocorrect_bigram(tests1, vocab, probs, \"Unit Test 1\", bigram_counts)\n",
    "# test_autocorrect_bigram(tests2, vocab, probs, \"Unit Test 2\", bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_autocorrect_bigram_min_edit(tests1, vocab, probs, \"Unit Test 1\", bigram_counts)\n",
    "# test_autocorrect_bigram_min_edit(tests2, vocab, probs, \"Unit Test 2\", bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This project is a implementation of a statistical auto-correct system. The architecture that has been developed give the accuracy around 52% - 55%. The improved verion of this architecture could get inference from previous word by using bi-gram pobabilities and min edit distance functionality provided further enhancement. Overall, this simple probability based auto-correct system performed okay. In order to get better performance one can go for deep learning based auto-correct systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References - \n",
    "- [How to Write a Spelling Corrector](https://norvig.com/spell-correct.html)\n",
    "- [Coursera NLP Specialization](https://www.coursera.org/learn/probabilistic-models-in-nlp/home/welcome)\n",
    "\n",
    "#### Happy Learning!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
